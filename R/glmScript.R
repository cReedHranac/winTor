########################################
#### GMB modeling script for winTor ####
## Author: C. Reed Hranac             ##
## Tested: 13 June 2020               ##
########################################

## this preforms the modeling for both the duration and mass components of the
## analysis by:
  ## loading the pre-cleaned data sets
  ## extracting the relevant co-variate data from the rasters generated in munge
  ## testing for spatial auto-correlation 
  ## running cross-validation models to identify potential outlier points
  ## running alternative models
  ## model selection through AIC
  ## reassessment of sptial auto-correlation 
  ## correction of auto-correlation (if needed)
  ## prediction of models back across the study extent


env.prior <- ls()
##packages
library(tidyverse); library(raster); library(spdep)

#### WINTER DURATION ####
## Data ##
dur <- read.csv("data/durationDataReferenced.csv")

## call in co-variates
env.names <- c("NA_dem", "NA_northing", "NA_nFrostyDays",
               "NA_nonGrowingDays", "NA_nDaysFreeze", "NA_OG1k")
env.stk <- raster::subset(
  raster::stack(list.files(win.dat, pattern = "NA_*", full.names = T)),
  env.names)

## amend estimates to a spatial data frame
coordinates(dur) <- ~ Long + Lat
proj4string(dur) <- proj4string(env.stk)
dur@data <- cbind(dur@data, raster::extract(env.stk, dur))

## try to determine if there are any points that'd fall within the
## same raster block (over inflate spatial AC)
dur.cellList <- cellFromXY(env.stk$NA_dem, dur)
anyDuplicated(dur.cellList) ## why does this say 3
which(duplicated(dur.cellList)) ## and this only id 2?

##examine
which(dur.cellList==dur.cellList[[3]]) ## 1 & 3
which(dur.cellList==dur.cellList[[37]]) ## 36 & 37

## average the 2 and create new entries
dur@data[c(1,3),] ## just choose one and remove
dur@data[c(36,37),] ## average and create new
(145+186)/2 # = 166
dur@data$winter.duration[[36]] <- 166

dur <- dur[-c(3,37),]
## checked and fixed

#### Checking spatial auto-corrilation ####
##create distance matrix
knn <- knearneigh(dur, k = 7) ## k set as the sqrt of nrow(dur)
nneighbor <- knn2nb(knn)
## plot to check things out
plot(dur)
plot(nneighbor, coords = coordinates(dur), col = "red", add = T)
#convert to weights list
nn.list <- nb2listw(nneighbor, style = "W")
## test
moran.mc(dur$winter.duration, nn.list, nsim = 999)
## yes there is correlation and presumably through all the potential
## covarites as well if they were added to the spdataframe

summary(dur@data)

spplot(dur, "winter.duration")
## distribution
hist(dur@data$winter.duration)
shapiro.test(dur@data$winter.duration)

#### preliminary lm to determine potential outlier points ####
mod.form <- function(x, coVar){
  ##Function to write out the formulas for me
  ## x is to be the item predicted
  ## covar is to be a list of the names of co-variates
  
  #univariate models
  l1 <- list()
  for(i in 1:length(coVar)){
    l1[[i]] <- as.formula(paste(x,"~",coVar[[i]]))
  }
  
  #with Norhting
  coVar2 <- coVar[-2]
  l2 <- list()
  for(i in 1:length(coVar2)){
    l2[[i]] <- as.formula(paste(x,"~",coVar[[2]],"+",coVar2[[i]]))
  }
  
  #with Northing and DEM
  coVar3 <- coVar[3:length(coVar)]
  l3 <- list()
  for(i in 1:length(coVar3)){
    l3[[i]] <- as.formula(paste(x,"~",coVar[[2]],"+",coVar[[1]],"+",coVar3[[i]]))
  }
  
  list.mods <- c(l1, l2, l3)
  
  return(list.mods)
}

library(caret)
cross.lm <- function(predictor, forms, x){
  ##function for preforming leave one out crossvalidation and determing
  ##which points are potentially over influencing the models
  # pred is to be what you want to predict
  # forms is a list of formulas generated by mod.form
  # x is the dataframe from which all the data comes from
  
  ##control statment for caret
  train_control <- trainControl(method="LOOCV", returnResamp = "all")
  
  # run the leave one out validation on each of the points and return pvals
  pred = list()
  for (i in 1:nrow(x)) {
    test = x[i,]
    train = x[-i,]
    mod = lm(forms, data=train)
    pred_fit = predict.lm(mod, test, interval="prediction", se.fit=TRUE)
    pred[[i]] <- cbind(pred_fit$fit,
                       se=pred_fit$se.fit,
                       res.scale=pred_fit$residual.scale,
                       df=pred_fit$df,
                       obs=test[,predictor])
  }
  
  pred = bind_rows(lapply(pred, as.data.frame))
  
  ## finds p val for observations, if outside of window of obs adj.pval will be low
  pval.pred <- pred %>% mutate(pred.sd = sqrt(se^2 + res.scale^2),
                               tval = (obs - fit) / pred.sd,
                               pval = 2*pt(abs(tval), df=df, lower.tail = FALSE)) %>%
    mutate(padj = p.adjust(pval)) %>% arrange(pval) %>%
    dplyr::filter(padj < 1)
  
  if(nrow(pval.pred) > 0){
    out <- list(df = as.data.frame(pval.pred),formula =  forms)  
  } else{
    out <- NA
  }
  
  return(out)
}

cross.wrapper <- function(predictor, coVarNames, df){
  ##function to wrapp and summarize cross.lm
  # predictor is the name of the variable to be predicted across
  # coVarNames is the list of predictor names
  # df is the name that all the junk comes from 
  
  #create model formulas
  model.formulas <- mod.form(x = predictor, coVar = coVarNames)
  
  #run crossvalidation protocol
  cross.list <- lapply(X = model.formulas,
                       FUN = cross.lm,
                       predictor = predictor,
                       x = df)
  #breaks when you have non-uniqe fitting values in the predictors
  #would need some rework with the predict dataframe managment
  # obs <- list()
  # v <- 1
  # for(i in 1:length(cross.list)){
  #   if(!is.na(cross.list[[i]])){
  #     for(j in 1:nrow(cross.list[[i]]$df))
  #       obs.sub <- cbind(df[which(df[,predictor] == cross.list[[i]]$df[j,"obs"]),],
  #                      pval = cross.list[[i]]$df[,"padj"])
  #     obs[[v]] <- list(formula = cross.list[[i]]$formula, records = obs.sub)
  #     v <- v+1
  #   }
  # }
  # 
  # return(obs)
  return(cross.list)
}

dur.points <- cross.wrapper(predictor = "winter.duration", 
                            coVarNames = env.names,
                            df = dur@data)

dur.points
## No points identified for concern
## points would need to have an adj p-value of < .5 for >5 of the models

#### GLM application ####
dur.mods <- lapply(mod.form("winter.duration",coVar = env.names),
                   FUN = glm, data = dur@data, family = "gaussian" )

## Extracting deviance as a vector from each
modnames = c("dem",
             "northing",
             "frost",
             "growing",
             "freeze",
             "OG",
             "north + dem",
             "north + frost",
             "north + growing",
             "north + freeze",
             "north + OG",
             "north + dem + frost",
             "north + dem + growing",
             "north + dem + freeze",
             "north + dem + OG")

dev.durModels <- as.data.frame(cbind(Modnames=unlist(modnames),
                                    deviance =unlist(lapply(dur.mods,
                                                      function(x){summary(x)$deviance}))))
#### AIC Model Selection ####
library(AICcmodavg)
dur.AIC <- aictab(dur.mods,
                  modnames = c("dem",
                               "northing",
                               "frost",
                               "growing",
                               "freeze",
                               "OG",
                               "north + dem",
                               "north + frost",
                               "north + growing",
                               "north + freeze",
                               "north + OG",
                               "north + dem + frost",
                               "north + dem + growing",
                               "north + dem + freeze",
                               "north + dem + OG"),
                  sort = F)

## write out the table to results
write.csv(dur.AIC,
          file =  file.path(win.res, 'durationResultsTable.csv'),
          row.names = F)


#### Re-assess Spatial residuals ####

## top model
dur.mod <- formula(dur.mods[[12]])

## add model residuals to spatial dataset
dur@data$RESID <- residuals(dur.mods[[12]])

## plot residuals
spplot(dur, "RESID")

## test residuals for spatial correlation
moran.mc(dur$RESID, listw = nn.list, nsim = 999)

# Monte-Carlo simulation of Moran I
# 
# data:  dur$RESID 
# weights: nn.list  
# number of simulations + 1: 1000 
# 
# statistic = -0.0036285, observed rank = 646, p-value =
#   0.354
# alternative hypothesis: greater

## lagrange multiplier - another test for spatial effects
lm.LMtests(dur.mods[[12]], listw = nn.list, test = "all")

# Lagrange multiplier diagnostics for spatial dependence
# 
# data:  
#   model: FUN(formula = X[[i]], family = "gaussian", data
#              = ..1)
# weights: nn.list
# 
# LMerr = 0.0027033, df = 1, p-value = 0.9585
# 
# 
# Lagrange multiplier diagnostics for spatial dependence
# 
# data:  
#   model: FUN(formula = X[[i]], family = "gaussian", data
#              = ..1)
# weights: nn.list
# 
# LMlag = 0.0048635, df = 1, p-value = 0.9444
# 
# 
# Lagrange multiplier diagnostics for spatial dependence
# 
# data:  
#   model: FUN(formula = X[[i]], family = "gaussian", data
#              = ..1)
# weights: nn.list
# 
# RLMerr = 2.0662e-07, df = 1, p-value = 0.9996
# 
# 
# Lagrange multiplier diagnostics for spatial dependence
# 
# data:  
#   model: FUN(formula = X[[i]], family = "gaussian", data
#              = ..1)
# weights: nn.list
# 
# RLMlag = 0.0021604, df = 1, p-value = 0.9629
# 
# 
# Lagrange multiplier diagnostics for spatial dependence
# 
# data:  
#   model: FUN(formula = X[[i]], family = "gaussian", data
#              = ..1)
# weights: nn.list
# 
# SARMA = 0.0048637, df = 2, p-value = 0.9976


## No spatial correlation of residuals found, we should be fine to 
## project this back across the distribution with out special consideration



#### Predictions and confidence bounds ####

## Prediction for estimate alone since prediction intervals aren't working
dur.top.mod <- dur.mods[[12]]
dur.predict <- raster::predict(object = env.stk,
                              model = dur.top.mod,
                              filename = file.path(win.res, "durationRaster_p.tif"),
                              format = "GTiff",
                              overwrite = T)


# glmRasterIntervals <- function(top.model, coVars, outName){
#   conffun <- function(x, data = NULL) {
#     v <- predict(object = x,
#                  newdata = data,
#                  type = "response",
#                  interval = "prediction")
#     
#     return(v)  
#   }
#   
#   conf.int <- raster::predict(object = coVars,
#                               model = top.model,
#                               data = coVars,
#                               fun = conffun,
#                               index = 1:3)
#   names(conf.int) <- c("p", "lwr", "upr")
#   
#   # writeRaster(x = conf.int,
#   #             filename = file.path(win.res, outName),
#   #             format = "GTiff",
#   #             bylayer = T,
#   #             suffix = "names",
#   #             overwrite = T)
#   return(conf.int) 
# }
# 
# 
# a <- glmRasterIntervals(top.model = dur.top.mod,
#                         coVars = env.stk,
#                         outName = "durationRaster")
# a
####  Mass  analysis ####
#### Data ####
mass <- read.csv("data/massDataReferenced.csv")

## Co-variates
env.names <- c("NA_dem", "NA_northing", "NA_nFrostyDays",
               "NA_nonGrowingDays", "NA_nDaysFreeze", "NA_OG1k")
env.stk <- raster::subset(stack(list.files(win.dat, pattern = "NA_*", full.names = T)), env.names)


## ammending to have co-variate data
coordinates(mass) <- ~ Long + Lat
proj4string(mass) <- proj4string(env.stk)
mass@data <- as.data.frame(cbind(mass, raster::extract(env.stk, mass)))

## try to determine if there are any points that'd fall within the
## same raster block (over inflate spatial AC)
mass.cellList <- cellFromXY(env.stk$NA_dem, mass)
anyDuplicated(mass.cellList) 
which(duplicated(mass.cellList)) ## 22 25 41 42
##examine
which(mass.cellList==mass.cellList[[22]]) ## 21 & 22
which(mass.cellList==mass.cellList[[25]]) ## 24 & 25
which(mass.cellList==mass.cellList[[41]]) ## 40 & 41
which(mass.cellList==mass.cellList[[42]]) ## 37 & 42
## average the 2 and create new entries
mass@data[c(21,22),] ## average
mass@data$avgMass[21] <- (mass@data$avgMass[21] + mass@data$avgMass[22])/2 
mass@data[c(24,25),] ## average
mass@data$avgMass[24] <- (mass@data$avgMass[24] + mass@data$avgMass[25])/2 
mass@data[c(40,41),]
mass@data$avgMass[41] <- (mass@data$avgMass[40] + mass@data$avgMass[41])/2 
mass@data[c(37,42),]
mass@data$avgMass[37] <- (mass@data$avgMass[37] + mass@data$avgMass[42])/2 

nrow(mass)
mass <- mass[-c(22,25,40,42),]
nrow(mass)
## checked and fixed

#### Checking spatial autocorrilation ####
##create distance matrix
knn <- knearneigh(mass, k = 7) ## k set as the sqrt of nrow(mass) rounded
nneighbor <- knn2nb(knn)
## plot to check things out
plot(mass)
plot(nneighbor, coords = coordinates(mass), col = "red", add = T)
#convert to weights list
nn.list <- nb2listw(nneighbor, style = "W")
## test
moran.mc(mass$avgMass, nn.list, nsim = 999)
## yes there is correlation and presumably through all the potential
## covarites as well

summary(mass@data)

spplot(mass, "avgMass")
## distribution
hist(mass@data$avgMass)
shapiro.test(mass@data$avgMass)
## just not normal  ## re check once outliers have been removed
#### preliminary lm to determine potential outlier points ####
mod.form <- function(x, coVar){
  ##Function to write out the formulas for me
  ## x is to be the item predicted
  ## covar is to be a list of the names oc covariates
  
  #univariate models
  l1 <- list()
  for(i in 1:length(coVar)){
    l1[[i]] <- as.formula(paste(x,"~",coVar[[i]]))
  }
  
  #with Norhting
  coVar2 <- coVar[-2]
  l2 <- list()
  for(i in 1:length(coVar2)){
    l2[[i]] <- as.formula(paste(x,"~",coVar[[2]],"+",coVar2[[i]]))
  }
  
  #with Northing and DEM
  coVar3 <- coVar[3:length(coVar)]
  l3 <- list()
  for(i in 1:length(coVar3)){
    l3[[i]] <- as.formula(paste(x,"~",coVar[[2]],"+",coVar[[1]],"+",coVar3[[i]]))
  }
  
  list.mods <- c(l1, l2, l3)
  
  return(list.mods)
}

library(caret)
cross.lm <- function(predictor, forms, x){
  ##function for preforming leave one out crossvalidation and determing
  ##which points are potentially over influencing the models
  # pred is to be what you want to predict
  # forms is a list of formulas generated by mod.form
  # x is the dataframe from which all the data comes from
  
  ##control statment for caret
  train_control <- trainControl(method="LOOCV", returnResamp = "all")
  
  # run the leave one out validation on each of the points and return pvals
  pred = list()
  for (i in 1:nrow(x)) {
    test = x[i,]
    train = x[-i,]
    mod = lm(forms, data=train)
    pred_fit = predict.lm(mod, test, interval="prediction", se.fit=TRUE)
    pred[[i]] <- cbind(pred_fit$fit,
                       se=pred_fit$se.fit,
                       res.scale=pred_fit$residual.scale,
                       df=pred_fit$df,
                       obs=test[,predictor])
  }
  
  pred = bind_rows(lapply(pred, as.data.frame))
  
  ## finds p val for observations, if outside of window of obs adj.pval will be low
  pval.pred <- pred %>% mutate(pred.sd = sqrt(se^2 + res.scale^2),
                               tval = (obs - fit) / pred.sd,
                               pval = 2*pt(abs(tval), df=df, lower.tail = FALSE)) %>%
    mutate(padj = p.adjust(pval)) %>% arrange(pval) %>%
    dplyr::filter(padj < 1)
  
  if(nrow(pval.pred) > 0){
    out <- list(df = as.data.frame(pval.pred),formula =  forms)  
  } else{
    out <- NA
  }
  
  return(out)
}

cross.wrapper <- function(predictor, coVarNames, df){
  ##function to wrapp and summarize cross.lm
  # predictor is the name of the variable to be predicted across
  # coVarNames is the list of predictor names
  # df is the name that all the junk comes from 
  
  #create model formulas
  model.formulas <- mod.form(x = predictor, coVar = coVarNames)
  
  #run crossvalidation protocol
  cross.list <- lapply(X = model.formulas,
                       FUN = cross.lm,
                       predictor = predictor,
                       x = df)
  #breaks when you have non-uniqe fitting values in the predictors
  #would need some rework with the predict dataframe managment
  # obs <- list()
  # v <- 1
  # for(i in 1:length(cross.list)){
  #   if(!is.na(cross.list[[i]])){
  #     for(j in 1:nrow(cross.list[[i]]$df))
  #       obs.sub <- cbind(df[which(df[,predictor] == cross.list[[i]]$df[j,"obs"]),],
  #                      pval = cross.list[[i]]$df[,"padj"])
  #     obs[[v]] <- list(formula = cross.list[[i]]$formula, records = obs.sub)
  #     v <- v+1
  #   }
  # }
  # 
  # return(obs)
  return(cross.list)
}

mass.points <- cross.wrapper(predictor = "avgMass", 
                            coVarNames = env.names,
                            df = mass@data)

mass.points
## the 14.5g observation needs to be removed
mass <- mass[-which(mass$avgMass == 14.5),]

## rechecking for normalcy 
shapiro.test(mass$avgMass)
## normal enough


#### GLM application ####

mass.mods <- lapply(mod.form("avgMass",coVar = env.names),
                   FUN = glm, data = mass@data, family = "gaussian" )

## Extracting deviance as a vector from each
modnames = c("dem",
             "northing",
             "frost",
             "growing",
             "freeze",
             "OG",
             "north + dem",
             "north + frost",
             "north + growing",
             "north + freeze",
             "north + OG",
             "north + dem + frost",
             "north + dem + growing",
             "north + dem + freeze",
             "north + dem + OG")

dev.massModels <- as.data.frame(cbind(Modnames=unlist(modnames),
                                     deviance =unlist(lapply(mass.mods,
                                                             function(x){summary(x)$deviance}))))
#### AIC Model Selection ####
library(AICcmodavg)
mass.AIC <- aictab(mass.mods,
                  modnames = c("dem",
                               "northing",
                               "frost",
                               "growing",
                               "freeze",
                               "OG",
                               "north + dem",
                               "north + frost",
                               "north + growing",
                               "north + freeze",
                               "north + OG",
                               "north + dem + frost",
                               "north + dem + growing",
                               "north + dem + freeze",
                               "north + dem + OG"),
                  sort = F)

## write out the table to results
write.csv(mass.AIC,
          file =  file.path(win.res, 'massResultsTable.csv'),
          row.names = F)

#### Spatial residuals ####
mass.mod <- formula(mass.mods[[10]])
## add model residuals to spatial dataset
mass@data$RESID <- residuals(mass.mods[[10]])
## plot residuals
spplot(mass, "RESID")

## re-create distance matrix since an obs was removed
knn <- knearneigh(mass, k = 6) ## k set as the sqrt of nrow(mass) rounded
nneighbor <- knn2nb(knn)
## plot to check things out
plot(mass)
plot(nneighbor, coords = coordinates(mass), col = "red", add = T)
#convert to weights list
nn.list <- nb2listw(nneighbor, style = "W")
## test
moran.mc(mass$avgMass, nn.list, nsim = 999)

## test residuals for spatial correlation
moran.mc(mass$RESID, listw = nn.list, nsim = 999) 
## lagrange multiplier - another test for spatial effects
lm.LMtests(mass.mods[[10]], listw = nn.list, test = "all")

## significant auto-correlation among the residuals
#### attempts at spatial GLM ####
## Methodology adapted from: https://cran.r-project.org/web/packages/glmmfields/vignettes/spatial-glms.html
library(glmmfields)
options(mc.cores = parallel::detectCores())  
system.time(
mass_spatial <- glmmfields(mass.mod,
                          data = mass@data,
                          lat = "Lat", lon = "Long",
                          nknots = 6, iter = 10000, chains = 5,
                          prior_intercept = student_t(3, 0, 10), 
                          prior_beta = student_t(3, 0, 3),
                          prior_sigma = half_t(3, 0, 3),
                          prior_gp_theta = half_t(3, 0, 10),
                          prior_gp_sigma = half_t(3, 0, 3),
                          seed = 123,
                          control = list(adapt_delta = 0.99,
                                         max_treedepth = 15)# passed to rstan::sampling()
))

plot(mass_spatial, type = "spatial-residual", link = TRUE) +
  geom_point(size = 3)
## looks much better
plot(mass_spatial, type = "residual-vs-fitted")
plot(mass_spatial, type = "prediction", link = FALSE) +
  viridis::scale_colour_viridis() +
  geom_point(size = 3)

#### Predictions and confidence bounds ####
## create layers for lat and long
env.stk$Long <- xFromCell(env.stk[[1]], cell = 1:ncell(env.stk))
env.stk$Lat <- yFromCell(env.stk[[1]],  cell = 1:ncell(env.stk))

## our prediction function - returns matrix of 3 columns
conffun <- function(model, data = NULL, iter='all') {
  v <- predict(object = model,
               newdata = data,
               type = "response",
               interval = "prediction",
               iter = iter)
  return(as.matrix(v))  
}

## change raster options to have way lower chunksize to help
## fit into memory. Can maybe increase this?
raster::rasterOptions(chunksize=1e5)
rasterOptions(memfrac = .3); rasterOptions(maxmemory = 1e+08)
library(cluster)
library(parallel)


## Use for prediction to full
beginCluster(2) ##can hand way too long depending on the number of cores
system.time({
  r.prob.Cluster<-clusterR(env.stk, fun=predict, args=list(model=mass_spatial,
                                                         fun=conffun,
                                                         progress='text',
                                                         index=1:3,
                                                         iter=1000))
})
endCluster() #delete the cluster


names(r.prob.Cluster) <- c("p", "lwr", "upr")
writeRaster(r.prob.Cluster,
            filename = file.path(win.res, "mass.tif"),
            format = "GTiff",
            bylayer = T,
            suffix = "names",
            overwrite = T)

#### Variogram of the top models ####
library(gstat)

#top model mass
mass.mod

v <- variogram(mass.mod,
               data = mass)
v.exp = fit.variogram(v, vgm("Exp"), fit.kappa = T)
v.mat <- fit.variogram(v, vgm("Mat"), fit.kappa = T)
v.sph <- fit.variogram(v, vgm("Sph"), fit.kappa = T)

par(mfrow = c(1,3))

plot(v, v.exp)
plot(v, v.mat)
plot(v, v.sph)

library(gridExtra)
Variogram.mass <- grid.arrange(plot(v, v.exp),
                              plot(v, v.mat),
                              plot(v, v.sph),
                              ncol = 1)

ggsave( file.path(win.res,"fig", "varioMass.png"),
        plot = Variogram.mass)

#### Clean up script items ####
env.post <- ls()
to.remove <- env.post[env.post %!in% env.prior]
rm(list=to.remove); rm(env.post, to.remove)
