#######################################
### GMB modling script for winTor###
#######################################

### Re-work of the linear modeling scripts to use gbm methodology

## Script is currently in the sandbox stage working on different methodology 
## conserving the first portion of the LM crossvalidation to remove outliers

#### Extra Paths####
# if (!exists('base.path')) {
#   if(.Platform$"OS.type" == "windows"){
#     base.path = file.path("D:", "Dropbox", "wintor_aux")
#   } else {
#     base.path = "~/Dropbox/winTor_aux"
#   }
# }

## path for new laptop since there are no partitions 
base.path = file.path("C:","Users","chran","Dropbox","winTor_aux")

win.dat <- file.path(base.path, "data")
win.res <- file.path(base.path, "Results")
## The %!in% opperator 
'%!in%' <- function(x,y)!('%in%'(x,y))

##packages
library(tidyverse);library(gbm); library(raster); library(spdep)
#### Data ####
dur <- read.csv("data/durationDataReferenced.csv")
mass <- read.csv("data/massDataReferenced.csv")

## Co-variates
env.names <- c("NA_dem", "NA_northing", "NA_nFrostyDays",
               "NA_nonGrowingDays", "NA_nDaysFreeze", "NA_OG1k")
env.stk <- raster::subset(stack(list.files(win.dat, pattern = "NA_*", full.names = T)), env.names)


## ammending to have co-variate data
coordinates(mass) <- ~ Long + Lat
proj4string(mass) <- proj4string(env.stk)
mass.df <- as.data.frame(cbind(mass, raster::extract(env.stk, mass)))

coordinates(dur) <- ~ Long + Lat
proj4string(dur) <- proj4string(env.stk)
dur@data <- cbind(dur@data, raster::extract(env.stk, dur))

## try to determine if there are any points that'd fall within the
## same raster block (over inflate spatial AC)
dur.cellList <- cellFromXY(env.stk$NA_dem, dur)
anyDuplicated(dur.cellList) ## why does this say 3
which(duplicated(dur.cellList)) ## and this only id 2?

##examine
which(dur.cellList==dur.cellList[[3]]) ## 1 & 3
which(dur.cellList==dur.cellList[[37]]) ## 36 & 37

## average the 2 and create new entries
dur@data[c(1,3),] ## just choose one and remove
dur@data[c(36,37),] ## average and create new
(145+186)/2 # = 166
dur@data$winter.duration[[36]] <- 166

dur <- dur[-c(3,37),]
## checked and fixed

#### Checking spatial autocorrilation ####
##create distance matrix
knn <- knearneigh(dur, k = 7) ## k set as the sqrt of nrow(dur)
nneighbor <- knn2nb(knn)
## plot to check things out
plot(dur)
plot(nneighbor, coords = coordinates(dur), col = "red", add = T)
#convert to weights list
nn.list <- nb2listw(nneighbor, style = "W")
## test
moran.mc(dur$winter.duration, nn.list, nsim = 999)
## yes there is correlation and presumably through all the potential
## covarites as well if they were added to the spdataframe

summary(dur@data)

spplot(dur, "winter.duration")
## distribution
hist(dur@data$winter.duration)
shapiro.test(dur@data$winter.duration)

#### preliminary lm to determine potential outlier points ####
mod.form <- function(x, coVar){
  ##Function to write out the formulas for me
  ## x is to be the item predicted
  ## covar is to be a list of the names oc covariates
  
  #univariate models
  l1 <- list()
  for(i in 1:length(coVar)){
    l1[[i]] <- as.formula(paste(x,"~",coVar[[i]]))
  }
  
  #with Norhting
  coVar2 <- coVar[-2]
  l2 <- list()
  for(i in 1:length(coVar2)){
    l2[[i]] <- as.formula(paste(x,"~",coVar[[2]],"+",coVar2[[i]]))
  }
  
  #with Northing and DEM
  coVar3 <- coVar[3:length(coVar)]
  l3 <- list()
  for(i in 1:length(coVar3)){
    l3[[i]] <- as.formula(paste(x,"~",coVar[[2]],"+",coVar[[1]],"+",coVar3[[i]]))
  }
  
  list.mods <- c(l1, l2, l3)
  
  return(list.mods)
}

library(caret)
cross.lm <- function(predictor, forms, x){
  ##function for preforming leave one out crossvalidation and determing
  ##which points are potentially over influencing the models
  # pred is to be what you want to predict
  # forms is a list of formulas generated by mod.form
  # x is the dataframe from which all the data comes from
  
  ##control statment for caret
  train_control <- trainControl(method="LOOCV", returnResamp = "all")
  
  # run the leave one out validation on each of the points and return pvals
  pred = list()
  for (i in 1:nrow(x)) {
    test = x[i,]
    train = x[-i,]
    mod = lm(forms, data=train)
    pred_fit = predict.lm(mod, test, interval="prediction", se.fit=TRUE)
    pred[[i]] <- cbind(pred_fit$fit,
                       se=pred_fit$se.fit,
                       res.scale=pred_fit$residual.scale,
                       df=pred_fit$df,
                       obs=test[,predictor])
  }
  
  pred = bind_rows(lapply(pred, as.data.frame))
  
  ## finds p val for observations, if outside of window of obs adj.pval will be low
  pval.pred <- pred %>% mutate(pred.sd = sqrt(se^2 + res.scale^2),
                               tval = (obs - fit) / pred.sd,
                               pval = 2*pt(abs(tval), df=df, lower.tail = FALSE)) %>%
    mutate(padj = p.adjust(pval)) %>% arrange(pval) %>%
    dplyr::filter(padj < 1)
  
  if(nrow(pval.pred) > 0){
    out <- list(df = as.data.frame(pval.pred),formula =  forms)  
  } else{
    out <- NA
  }
  
  return(out)
}

cross.wrapper <- function(predictor, coVarNames, df){
  ##function to wrapp and summarize cross.lm
  # predictor is the name of the variable to be predicted across
  # coVarNames is the list of predictor names
  # df is the name that all the junk comes from 
  
  #create model formulas
  model.formulas <- mod.form(x = predictor, coVar = coVarNames)
  
  #run crossvalidation protocol
  cross.list <- lapply(X = model.formulas,
                       FUN = cross.lm,
                       predictor = predictor,
                       x = df)
  #breaks when you have non-uniqe fitting values in the predictors
  #would need some rework with the predict dataframe managment
  # obs <- list()
  # v <- 1
  # for(i in 1:length(cross.list)){
  #   if(!is.na(cross.list[[i]])){
  #     for(j in 1:nrow(cross.list[[i]]$df))
  #       obs.sub <- cbind(df[which(df[,predictor] == cross.list[[i]]$df[j,"obs"]),],
  #                      pval = cross.list[[i]]$df[,"padj"])
  #     obs[[v]] <- list(formula = cross.list[[i]]$formula, records = obs.sub)
  #     v <- v+1
  #   }
  # }
  # 
  # return(obs)
  return(cross.list)
}

dur.points <- cross.wrapper(predictor = "winter.duration", 
                            coVarNames = env.names,
                            df = dur@data)

dur.points
## No points identified for concern

#### GLM application ####

dur.mods <- lapply(mod.form("winter.duration",coVar = env.names),
                   FUN = glm, data = dur@data, family = "gaussian" )

## Extracting deviance as a vector from each
modnames = c("dem",
             "northing",
             "frost",
             "growing",
             "freeze",
             "OG",
             "north + dem",
             "north + frost",
             "north + growing",
             "north + freeze",
             "north + OG",
             "north + dem + frost",
             "north + dem + growing",
             "north + dem + freeze",
             "north + dem + OG")

dev.durModels <- as.data.frame(cbind(Modnames=unlist(modnames),
                                    deviance =unlist(lapply(dur.mods,
                                                      function(x){summary(x)$deviance}))))
#### AIC Model Selection ####
library(AICcmodavg)
dur.AIC <- aictab(dur.mods,
                  modnames = c("dem",
                               "northing",
                               "frost",
                               "growing",
                               "freeze",
                               "OG",
                               "north + dem",
                               "north + frost",
                               "north + growing",
                               "north + freeze",
                               "north + OG",
                               "north + dem + frost",
                               "north + dem + growing",
                               "north + dem + freeze",
                               "north + dem + OG"))
#### Spatial residuals ####
dur.mod <- formula(dur.mods[[12]])
## add model residuals to spatial dataset
dur@data$RESID <- residuals(dur.mods[[12]])
## plot residuals
spplot(dur, "RESID")
## test residuals for spatial correlation
moran.mc(dur$RESID, listw = nn.list, nsim = 999) 
## lagrange multiplier - another test for spatial effects
lm.LMtests(dur.mods[[12]], listw = nn.list, test = "all")

#### Predictions and confidence bounds ####
glmRasterIntervals <- function(model, coVars, outName){
  conffun <- function(model, data = NULL) {
    v <- predict.glm(object = model,
                     newdata = data,
                     type = "response",
                     interval = "prediction")
    
  return(v)  
  }
  
  conf.int <- raster::predict(model, object = coVars, fun = conffun, index = 1:3)
  names(conf.int) <- c("p", "lwr", "upr")
  
  writeRaster(x = conf.int,
              filename = file.path(win.res, outName),
              format = "GTiff",
              bylayer = T,
              suffix = "names",
              overwrite = T)
  
}

dur.top.form <- mod.form("winter.duration",coVar = env.names)[[12]]
dur.top.mod <- dur.mods[[12]]
a <- glmRasterIntervals(dur.top.mod,
                  coVars = env.stk,
                  outName = "durationRaster")
####  Mass  analysis ####
#### Data ####
mass <- read.csv("data/massDataReferenced.csv")

## Co-variates
env.names <- c("NA_dem", "NA_northing", "NA_nFrostyDays",
               "NA_nonGrowingDays", "NA_nDaysFreeze", "NA_OG1k")
env.stk <- raster::subset(stack(list.files(win.dat, pattern = "NA_*", full.names = T)), env.names)


## ammending to have co-variate data
coordinates(mass) <- ~ Long + Lat
proj4string(mass) <- proj4string(env.stk)
mass@data <- as.data.frame(cbind(mass, raster::extract(env.stk, mass)))

## try to determine if there are any points that'd fall within the
## same raster block (over inflate spatial AC)
mass.cellList <- cellFromXY(env.stk$NA_dem, mass)
anyDuplicated(mass.cellList) 
which(duplicated(mass.cellList)) ## 22 25 41 42
##examine
which(mass.cellList==mass.cellList[[22]]) ## 21 & 22
which(mass.cellList==mass.cellList[[25]]) ## 24 & 25
which(mass.cellList==mass.cellList[[41]]) ## 40 & 41
which(mass.cellList==mass.cellList[[42]]) ## 37 & 42
## average the 2 and create new entries
mass@data[c(21,22),] ## average
mass@data$avgMass[21] <- (mass@data$avgMass[21] + mass@data$avgMass[22])/2 
mass@data[c(24,25),] ## average
mass@data$avgMass[24] <- (mass@data$avgMass[24] + mass@data$avgMass[25])/2 
mass@data[c(40,41),]
mass@data$avgMass[41] <- (mass@data$avgMass[40] + mass@data$avgMass[41])/2 
mass@data[c(37,42),]
mass@data$avgMass[37] <- (mass@data$avgMass[37] + mass@data$avgMass[42])/2 

nrow(mass)
mass <- mass[-c(22,25,40,42),]
nrow(mass)
## checked and fixed

#### Checking spatial autocorrilation ####
##create distance matrix
knn <- knearneigh(mass, k = 7) ## k set as the sqrt of nrow(mass) rounded
nneighbor <- knn2nb(knn)
## plot to check things out
plot(mass)
plot(nneighbor, coords = coordinates(mass), col = "red", add = T)
#convert to weights list
nn.list <- nb2listw(nneighbor, style = "W")
## test
moran.mc(mass$avgMass, nn.list, nsim = 999)
## yes there is correlation and presumably through all the potential
## covarites as well

summary(mass@data)

spplot(mass, "avgMass")
## distribution
hist(mass@data$avgMass)
shapiro.test(mass@data$avgMass)
## just not normal  ## re check once outliers have been removed
#### preliminary lm to determine potential outlier points ####
mod.form <- function(x, coVar){
  ##Function to write out the formulas for me
  ## x is to be the item predicted
  ## covar is to be a list of the names oc covariates
  
  #univariate models
  l1 <- list()
  for(i in 1:length(coVar)){
    l1[[i]] <- as.formula(paste(x,"~",coVar[[i]]))
  }
  
  #with Norhting
  coVar2 <- coVar[-2]
  l2 <- list()
  for(i in 1:length(coVar2)){
    l2[[i]] <- as.formula(paste(x,"~",coVar[[2]],"+",coVar2[[i]]))
  }
  
  #with Northing and DEM
  coVar3 <- coVar[3:length(coVar)]
  l3 <- list()
  for(i in 1:length(coVar3)){
    l3[[i]] <- as.formula(paste(x,"~",coVar[[2]],"+",coVar[[1]],"+",coVar3[[i]]))
  }
  
  list.mods <- c(l1, l2, l3)
  
  return(list.mods)
}

library(caret)
cross.lm <- function(predictor, forms, x){
  ##function for preforming leave one out crossvalidation and determing
  ##which points are potentially over influencing the models
  # pred is to be what you want to predict
  # forms is a list of formulas generated by mod.form
  # x is the dataframe from which all the data comes from
  
  ##control statment for caret
  train_control <- trainControl(method="LOOCV", returnResamp = "all")
  
  # run the leave one out validation on each of the points and return pvals
  pred = list()
  for (i in 1:nrow(x)) {
    test = x[i,]
    train = x[-i,]
    mod = lm(forms, data=train)
    pred_fit = predict.lm(mod, test, interval="prediction", se.fit=TRUE)
    pred[[i]] <- cbind(pred_fit$fit,
                       se=pred_fit$se.fit,
                       res.scale=pred_fit$residual.scale,
                       df=pred_fit$df,
                       obs=test[,predictor])
  }
  
  pred = bind_rows(lapply(pred, as.data.frame))
  
  ## finds p val for observations, if outside of window of obs adj.pval will be low
  pval.pred <- pred %>% mutate(pred.sd = sqrt(se^2 + res.scale^2),
                               tval = (obs - fit) / pred.sd,
                               pval = 2*pt(abs(tval), df=df, lower.tail = FALSE)) %>%
    mutate(padj = p.adjust(pval)) %>% arrange(pval) %>%
    dplyr::filter(padj < 1)
  
  if(nrow(pval.pred) > 0){
    out <- list(df = as.data.frame(pval.pred),formula =  forms)  
  } else{
    out <- NA
  }
  
  return(out)
}

cross.wrapper <- function(predictor, coVarNames, df){
  ##function to wrapp and summarize cross.lm
  # predictor is the name of the variable to be predicted across
  # coVarNames is the list of predictor names
  # df is the name that all the junk comes from 
  
  #create model formulas
  model.formulas <- mod.form(x = predictor, coVar = coVarNames)
  
  #run crossvalidation protocol
  cross.list <- lapply(X = model.formulas,
                       FUN = cross.lm,
                       predictor = predictor,
                       x = df)
  #breaks when you have non-uniqe fitting values in the predictors
  #would need some rework with the predict dataframe managment
  # obs <- list()
  # v <- 1
  # for(i in 1:length(cross.list)){
  #   if(!is.na(cross.list[[i]])){
  #     for(j in 1:nrow(cross.list[[i]]$df))
  #       obs.sub <- cbind(df[which(df[,predictor] == cross.list[[i]]$df[j,"obs"]),],
  #                      pval = cross.list[[i]]$df[,"padj"])
  #     obs[[v]] <- list(formula = cross.list[[i]]$formula, records = obs.sub)
  #     v <- v+1
  #   }
  # }
  # 
  # return(obs)
  return(cross.list)
}

mass.points <- cross.wrapper(predictor = "avgMass", 
                            coVarNames = env.names,
                            df = mass@data)

mass.points
## the 14.5g observation needs to be removed
which(mass$avgMass == 14.5)
mass <- mass[-10,]

## rechecking for normalcy 
shapiro.test(mass$avgMass)
## normal enough for now


#### GLM application ####

mass.mods <- lapply(mod.form("avgMass",coVar = env.names),
                   FUN = glm, data = mass@data, family = "gaussian" )

## Extracting deviance as a vector from each
modnames = c("dem",
             "northing",
             "frost",
             "growing",
             "freeze",
             "OG",
             "north + dem",
             "north + frost",
             "north + growing",
             "north + freeze",
             "north + OG",
             "north + dem + frost",
             "north + dem + growing",
             "north + dem + freeze",
             "north + dem + OG")

dev.massModels <- as.data.frame(cbind(Modnames=unlist(modnames),
                                     deviance =unlist(lapply(mass.mods,
                                                             function(x){summary(x)$deviance}))))
#### AIC Model Selection ####
library(AICcmodavg)
mass.AIC <- aictab(mass.mods,
                  modnames = c("dem",
                               "northing",
                               "frost",
                               "growing",
                               "freeze",
                               "OG",
                               "north + dem",
                               "north + frost",
                               "north + growing",
                               "north + freeze",
                               "north + OG",
                               "north + dem + frost",
                               "north + dem + growing",
                               "north + dem + freeze",
                               "north + dem + OG"))
#### Spatial residuals ####
mass.mod <- formula(mass.mods[[10]])
## add model residuals to spatial dataset
mass@data$RESID <- residuals(mass.mods[[10]])
## plot residuals
spplot(mass, "RESID")

## re-create distance matrix since an obs was removed
knn <- knearneigh(mass, k = 6) ## k set as the sqrt of nrow(mass) rounded
nneighbor <- knn2nb(knn)
## plot to check things out
plot(mass)
plot(nneighbor, coords = coordinates(mass), col = "red", add = T)
#convert to weights list
nn.list <- nb2listw(nneighbor, style = "W")
## test
moran.mc(mass$avgMass, nn.list, nsim = 999)

## test residuals for spatial correlation
moran.mc(mass$RESID, listw = nn.list, nsim = 999) 
## lagrange multiplier - another test for spatial effects
lm.LMtests(mass.mods[[10]], listw = nn.list, test = "all")

## significant auto-correlation among the residuals
#### attempts at spatial GLM ####
## Methodology adapted from: https://cran.r-project.org/web/packages/glmmfields/vignettes/spatial-glms.html
library(glmmfields)
options(mc.cores = parallel::detectCores())  
system.time(
mass_spatial <- glmmfields(mass.mod,
                          data = mass@data,
                          lat = "Lat", lon = "Long",
                          nknots = 6, iter = 10000, chains = 5,
                          prior_intercept = student_t(3, 0, 10), 
                          prior_beta = student_t(3, 0, 3),
                          prior_sigma = half_t(3, 0, 3),
                          prior_gp_theta = half_t(3, 0, 10),
                          prior_gp_sigma = half_t(3, 0, 3),
                          seed = 123,
                          control = list(adapt_delta = 0.99,
                                         max_treedepth = 15)# passed to rstan::sampling()
))

plot(mass_spatial, type = "spatial-residual", link = TRUE) +
  geom_point(size = 3)
## looks much better
plot(mass_spatial, type = "residual-vs-fitted")
plot(mass_spatial, type = "prediction", link = FALSE) +
  viridis::scale_colour_viridis() +
  geom_point(size = 3)
# link scale:
p <- predict(mass_spatial)
head(p)
p <- predict(mass_spatial, type = "response")
head(p)
## what am I missing here? shouldnt there be a difference?
# prediction intervals on new observations (include observation error):
p <- predict(mass_spatial, type = "response", interval = "prediction")
## ^ I presume that this is what I should use?
 
## See if we can adapt

# glmfieldsRasterIntervals <- function(top.model, coVars, outName){
#   conffun <- function(model, data = NULL) {
#     v <- predict(object = top.model,
#                      newdata = coVars,
#                      type = "response", 
#                      interval = "prediction")
#     # pred.out <- cbind(p=v$fit,
#     #                   lwr = (v$fit - (v$se.fit*1.96)),
#     #                   upr = (v$fit + (v$se.fit*1.95)))
#     
#     return(v)  
#   }
#   
#   conf.int <- raster::predict(top.model, object = coVars, fun = conffun, index = 1:3)
#   names(conf.int) <- c("p", "lwr", "upr")
#   
#   writeRaster(x = conf.int,
#               filename = file.path(win.res, outName),
#               format = "GTiff",
#               bylayer = T,
#               suffix = "names",
#               overwrite = T)
#   
# }
# 
# 
# massPred <- glmfieldsRasterIntervals(mass_spatial, 
#                                      coVars = env.stk,
#                                      outName = "massSpatialRaster")

# a <- raster::predict(object = env.stk, model = mass_spatial)
# a2 <- predict(object = mass_spatial,
#               newdata = env.stk,
#               type = "response",
#               interval = "prediction",
#               na.action = "pass")
## prediction is being a jerk...
## work around could be to:
  ## turn to data frame
  ## add cell column
  ## add columns for lat and long
  ## remove all non-complete rows
  ## predict across through the standard function
  ## fill back the predicted values into the raster cells manually
# env.df <- as.data.frame(rasterToPoints(env.stk))
# env.df <- env.df %>%
#   dplyr::select(x, y, NA_northing, NA_nDaysFreeze)
# colnames(env.df)[1:2] <- c("Long", "Lat")
# mass.pred <- predict(object = mass_spatial,
#                      newdata = env.df,
#                      type = "response",
#                      interval = "prediction",
#                      na.action = "pass")
# ^ fail


## Raster method attempt 
env.stk$Long <- xFromCell(env.stk[[1]], cell = 1:ncell(env.stk))
env.stk$Lat <- yFromCell(env.stk[[1]],  cell = 1:ncell(env.stk))

# test <- raster::predict(env.stk, model =mass_spatial )
## Fail. memory overload 15777 Gb

## lapply acorss
env.df <- as.data.frame(env.stk) ## create dataframe
env.df$cell <- 1:nrow(env.stk) ## add cell reference
env.slim <- env.df[complete.cases(env.df),] ## select complete cases only
# env.slim <- sapply(env.slim, as.numeric) ## doesnt solve
# env.pred <- apply(env.slim, 1, predict,
#                   object = mass_spatial,
#                   type = "response",
#                   interval = "prediction")
## ^ doesnt work

my.predict <- function(x){
  predict(object = mass_spatial,
                      newdata = as.tbl(x),
                      type = "response",
                      interval = "prediction"
          )
}

env.pred <- apply(env.slim, 1, my.predict)
# Error in UseMethod("as.tbl") : 
#  no applicable method for 'as.tbl' applied to an object of class
#  "c('double', 'numeric')"

## I dont understand why this won't work. the env.slim data can be coheresed 
## with the as.tbl arg
  
#### Predictions and confidence bounds ####
glmRasterIntervals <- function(model, coVars, outName){
  conffun <- function(model, data = NULL) {
    v <- predict.glm(object = model,
                     newdata = data,
                     type = "link", 
                     se.fit = T)
    pred.out <- cbind(p=v$fit,
                      lwr = (v$fit - (v$se.fit*1.96)),
                      upr = (v$fit + (v$se.fit*1.95)))
    
    return(pred.out)  
  }
  
  conf.int <- raster::predict(model, object = coVars, fun = conffun, index = 1:3)
  names(conf.int) <- c("p", "lwr", "upr")
  
  writeRaster(x = conf.int,
              filename = file.path(win.res, outName),
              format = "GTiff",
              bylayer = T,
              suffix = "names",
              overwrite = T)
  
}

mass.top.form <- mod.form("avgMass",coVar = env.names)[[10]]
mass.top.mod <- mass.mods[[10]]
a <- glmRasterIntervals(mass.top.mod,
                        coVars = env.stk,
                        outName = "massRaster")


ggplot(mass@data, aes(Long, Lat, colour = RESID)) +
  scale_color_gradient2() +
  geom_point(size = 3)



#### Variogram of the top models ####
library(gstat)

#top model mass
mass.mod

v <- variogram(mass.mod,
               data = mass)
v.exp = fit.variogram(v, vgm("Exp"), fit.kappa = T)
v.mat <- fit.variogram(v, vgm("Mat"), fit.kappa = T)
v.sph <- fit.variogram(v, vgm("Sph"), fit.kappa = T)

par(mfrow = c(1,3))

plot(v, v.exp)
plot(v, v.mat)
plot(v, v.sph)

library(gridExtra)
Variogram.mass <- grid.arrange(plot(v, v.exp),
                              plot(v, v.mat),
                              plot(v, v.sph),
                              ncol = 1)

ggsave( file.path(win.res,"fig", "varioMass.pdf"),
        plot = Variogram.mass,
        device = cairo_pdf)

#### Sandbox Below ####

#
mass.df <- mass.df[-which(mass.df$avgMass==14.5),]

#### Linear model application ####

dur.mods <- lapply(mod.form("winter.duration",coVar = env.names),
                   FUN = lm, data = dur.df )

mass.mods <- lapply(mod.form("avgMass",coVar = env.names),
                    FUN = lm, data = mass.df )

#### gbm ####

## Critical for model to operate apparently
dur.sub <- dur.df %>%
  dplyr::select(winter.duration,NA_dem, NA_northing, NA_nFrostyDays)


## set seed
set.seed(9)
## tain model
dur.fit <- gbm(
  formula = winter.duration ~ .,
  distribution = "gaussian",
  data = dur.sub,
  n.trees = 5000,
  interaction.depth = 3, 
  bag.fraction = 2,
  shrinkage = 0.05,
  cv.folds = 10
)

## print and examine
print(dur.fit)
min_MSE <- which.min(dur.fit$cv.error)
sqrt(dur.fit$cv.error[min_MSE])
gbm.perf(dur.fit, method = "cv")

### working on creating a function/loop for doing model selection across all

hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

random_index <- sample(1:nrow(dur.sub), nrow(dur.sub))
random_dur_train <- dur.sub[random_index, ]

# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(9)
  
  # train model
  gbm.tune <- gbm(
    formula = winter.duration ~ .,
    distribution = "gaussian",
    data = dur.sub,
    n.trees = 5000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .9,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)




#### dismo ####
library(dismo)
dur.dis <- gbm.step(data = dur.df,
                    gbm.x = 6:8,
                    gbm.y = 4,
                    family = "gaussian",
                    tree.complexity = 2,
                    learning.rate = 0.1,
                    bag.fraction = .75,
                    n.trees = 10000)
### this doesn't appear to work at all...



#### from the Crase et al 2012 paper ####
require(gbm); require(raster); require(dismo)
## no mention of hte brt functions or the model_funtions in the paper
## must be from what was eventually to become the dismo package?
head(dur.df)
set.seed(9)
env.brt <- gbm.step(data = dur.df,
                    gbm.x = 6:8,
                    gbm.y = 4,
                    family = "gaussian", ## gaussian enough at least by the Shapiro-Wilks,
                    tree.complexity = 3,
                    learning.rate = .002,
                    n.trees = 5000,
                    bag.fraction = .5)
summary(env.brt)
##RAC model 
## what should the background be
rast.brt <- calc(env.stk[[1]], function(x) ifelse(!is.na(x), NA, NA))
xy.res.brt <- cbind(dur.df[,12:13], resid(env.brt))
rast.brt[cellFromXY(rast.brt,xy.res.brt)] <- xy.res.brt[,3]
focal.rast.brt <- raster::focal(rast.brt,
                                w=matrix(1,25,25),
                                ngb = 25,
                                fun = mean,
                                na.rm = T)
plot(focal.rast.brt)
## Not positive this is doing it right but the funciton from the script doesn't exist
dur.df$resid <- focal.rast.brt[cellFromXY(rast.brt,xy.res.brt)]

rac.brt <- gbm.step(data = dur.df,
                    gbm.x = c(6:8,14),
                    gbm.y = 4,
                    family = "gaussian", 
                    tree.complexity = 3,
                    learning.rate = .002,
                    n.trees = 5000,
                    bag.fraction = .5,
                    n.folds = 10)
summary(rac.brt)


#### Mass ####
  
head(mass.df)
##check distribution
mass.shapW <- shapiro.test(mass.df$avgMass) ##normal enough


##unable to get a any trees to fit to our "best model" identified through the AUC model
## tree complexity is high to allow lots of interactions between the potential drivers
mass.brt <- gbm.step(data = mass.df,
                    gbm.x = c(4:9),
                    gbm.y = 1,
                    family = "gaussian", ## gaussian enough at least by the Shapiro-Wilks,
                    tree.complexity = 5,
                    learning.rate = .01,
                    n.trees = 100000,
                    bag.fraction = .75)
summary(mass.brt)
##From here I'm going to a sucessive model fit technique excluding the worst predictor variables
## first removing growing and OG as they were the lowest with 10%  
## decreased tree complexity to account for the reduction in number of variabals
mass2.brt <- gbm.step(data = mass.df,
                     gbm.x = c(4:6,8),
                     gbm.y = 1,
                     family = "gaussian", ## gaussian enough at least by the Shapiro-Wilks,
                     tree.complexity = 4,
                     learning.rate = .01,
                     n.trees = 100000,
                     bag.fraction = .75)
summary(mass2.brt)

## Northing was the next removed although it was close with frost
mass3.brt <- gbm.step(data = mass.df,
                      gbm.x = c(4,6,8),
                      gbm.y = 1,
                      family = "gaussian", ## gaussian enough at least by the Shapiro-Wilks,
                      tree.complexity = 4,
                      learning.rate = .01,
                      n.trees = 100000,
                      bag.fraction = .75)
summary(mass3.brt)
## again?
mass4.brt <- gbm.step(data = mass.df,
                      gbm.x = c(4,8),
                      gbm.y = 1,
                      family = "gaussian", ## gaussian enough at least by the Shapiro-Wilks,
                      tree.complexity = 3,
                      learning.rate = .01,
                      n.trees = 10000000,
                      bag.fraction = .75)
summary(mass4.brt)

##RAC model 
## what should the background be
Mrast.brt <- calc(env.stk[[1]], function(x) ifelse(!is.na(x), NA, NA))
xy.res.brt <- cbind(mass.df[,10:11], resid(mass.brt))
Mrast.brt[cellFromXY(Mrast.brt,xy.res.brt)] <- xy.res.brt[,3]
Mfocal.rast.brt <- raster::focal(Mrast.brt,
                                w=matrix(1,255,255),
                                ngb = 25,
                                fun = mean,
                                na.rm = T)
plot(Mfocal.rast.brt)
## Not positive this is doing it right but the funciton from the script doesn't exist
mass.df$resid <- Mfocal.rast.brt[cellFromXY(Mrast.brt,xy.res.brt)]

M.rac.brt <- gbm.step(data = mass.df,
                    gbm.x = c(5:8),
                    gbm.y = 1,
                    family = "gaussian", 
                    tree.complexity = 1,
                    learning.rate = .05,
                    n.trees = 1000000,
                    bag.fraction = .8,
                    n.folds = 10)
summary(rac.brt)
plot(rac.brt$residuals)
cor(dur.df$winter.duration, rac.brt$residuals)
####  Spatial GLMS ####
 ## Methodology adapted from: https://cran.r-project.org/web/packages/glmmfields/vignettes/spatial-glms.html
library(glmmfields)
options(mc.cores = parallel::detectCores())  

dur.glm <- glm(formula = winter.duration ~ NA_dem + NA_northing + NA_nFrostyDays,
               data = dur.df)
dur.glm
confint(dur.glm)
## checking for spatial auto corrilation
dur.df$glm_residuals <- residuals(dur.glm)
ggplot(dur.df, aes(Long, Lat, colour = glm_residuals)) +
  scale_color_gradient2() +
  geom_point(size = 3)
## spatial GLM
dur_spatial <- glmmfields(winter.duration ~ NA_dem + NA_northing + NA_nFrostyDays,
                        data = dur.df, family = ,
                        lat = "Lat", lon = "Long",
                        nknots = 12, iter = 10000, chains = 5,
                        prior_intercept = student_t(3, 0, 10), 
                        prior_beta = student_t(3, 0, 3),
                        prior_sigma = half_t(3, 0, 3),
                        prior_gp_theta = half_t(3, 0, 10),
                        prior_gp_sigma = half_t(3, 0, 3),
                        seed = 123,
                        control = list(adapt_delta = 0.99,
                                       max_treedepth = 15)# passed to rstan::sampling()
)

plot(dur_spatial, type = "residual-vs-fitted")
